\documentclass[11pt, A4, twoside]{report}
\usepackage[utf8]{inputenc}
%\usepackage[english]{babel}

% Needed for equation environments etc. https://www.overleaf.com/learn/latex/Aligning%20equations%20with%20amsmath
\usepackage{amsmath}

% Boldface printing for all characters in math mode with \vect{}
\usepackage{bm}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}



% non-tikz figures
\usepackage{graphicx}
\graphicspath{ {./figures/} }

% for subfigures
\usepackage{subcaption}

% ####################  recommended from how_to_tikzplot
\usepackage{tikz, pgfplots}
\pgfplotsset{compat=newest}

\newlength{\figheight}
\newlength{\figwidth}
% ####################
%
\usepgfplotslibrary{groupplots}

% Required for tables generated by python pandas
\usepackage{booktabs}

\usepackage[style=apa]{biblatex}
\addbibresource{thesis.bib}

\usepackage[toc,page]{appendix}

\usepackage{subfiles} % Best loaded last in the preamble according to https://www.overleaf.com/learn/latex/Multi-file_LaTeX_projects#The_subfiles_package

\DeclareMathOperator{\kl}{KL}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\inv}{Inv}
\DeclareMathOperator{\nig}{NIG}
% \DeclareMathOperator{\dif}{d}
\newcommand*\dif{\mathop{}\!\mathrm{d}}

\title{A Closer Look at Ensembling Methods for Approximating Bayesian Neural Networks}
\author{Philipp A. Hummel}
\date{\today}

\begin{document}

\begin{titlepage}
\maketitle
\end{titlepage}


\begin{abstract}
\subfile{chapters/00_abstract}
\end{abstract}


\tableofcontents

\bigskip

This document should contain the whole thesis in bullet points and figures. Especially questions asked and answers found should be here. The individiual chapter documents should then contain full text. So I can easily compile a document that is a short overview and the full document by just uncommenting some lines.



\chapter{Introduction}
\subfile{chapters/01_bullet_introduction}
% \subfile{chapters/01_introduction}





\chapter{Background}
\subfile{chapters/02_bullet_background}
% \subfile{chapters/02_background}





\chapter{Last Layer Bayesian Ensemble}
\subfile{chapters/03_bullet_llb_ensemble}
% \subfile{chapters/03_llb_ensemble}






\chapter{UCI Datasets and Benchmarks}
\subfile{chapters/04_bullet_ensembling_UCI}
% \subfile{chapters/04_ensembling_UCI}





\chapter{Comparing Ensembles with MCMC Posterior}
\subfile{chapters/05_bullet_comparison_to_mcmc}
% \subfile{chapters/05_comparison_to_mcmc}






\chapter{Conclusion}
\subfile{chapters/06_conclusion}






\chapter{Things to include somewhere}
\begin{itemize}
  \item It seems like for out of distribution data (i.e. when the IID assumption is violated) there are no guarantees for the bayesian theorems. But it does work reasonably well when using HMC (2018 | Sufficient Conditions for idealized Models to Have No Adversarial Examples | Gal, Smith). Maybe there is some interesting math because we are using discriminative models and not generative models?
  
  Answer:
  Well first, what do we mean by OOD? We mean out of the distribution of X, but still within the same distribution of $y|X$. Otherwise we indeed have no chance and no guarantees since $E[y|X]$ might change (e.g. the whole function might be shifted by +5). But what happens in a discriminative / conditional model when we get new data points from a different distribution of X? Well we have used the training data, to learn something about the posterior distribution over functions. If our parametric model was very flexible, the posterior distribution of functions should still be super wide outside the range of the X seen so far, but I see no principled problem in encountering these new data points and measuring test log likelihood on them. It all depends on our modelling assumptions about the function. Does it have global or local properties? When we assume global properties like linear functions, then we can learn a lot about OOD regions. When we instead assume local function properties, we cannot deduce much about regions that are far away from the training data, but there the predictive distribution should go back to the prior. The prior prediction might be bad, but at least it should have high uncertainty, and this might just be enough for practical purposes, where often we just want to halt automatic decision-making, when we encounter test data that is far away from the training data. Compare “Bayesian Data Analysis” 3rd edition 2014 Gelman, Chapter 14.1.
  \item Bayesian linear regression; when you want to have completely separate prior distributions for the weights and the noise standard deviation, you end up with a semi conjugate model. That means that the marginal posterior distributions for each variable are not tractable. However, the conditional posterior distributions are tractable. Then you can easily use a Gibbs sampler for computing the posterior by sampling. I have not seen it clearly stated, but it seems to me that the posterior predictive distribution in this case is also not analytically tractable.
  \item We should say something like ``Great care was taken to ensure that all models considered here are actually approximating the same posterior distribution''. Then we can explain that all models need to model the likelihood distribution's noise variance explicitly and that we chose to model the noise variance homoscedastically. When we try to make a map model find a local maximum of the same posterior (as HMC) then using L2 regularization can only work when we fix the noise variance beforehand. So in this case it was necessary to instead add a loss that is equivalent to the log prior
  \item What about modelling heteroscedasticity? I would expect that these models perform better (even when the noise is actually homoscedastic? Probably not, but in reality it probably rarely is actually homoscedastic.). When we only want to learn something about ensembling, using a homoscedastic model is not a problem. When we want to show that our model has superior performance (i.e. for the LLB ensemble case) it might be most honest if we also compared additionally to a heteroscedastic model. Or at least mention that future work should compare to a heteroscedastic model on real-world datasets, and that LLB is not easy to do in that case (but we can imagine doing a VI approach for that case, where it would likely be tractable to model weight covariances, since we are only concerned with the last layer. (So this would only be a multivariate Normal distribution. Can we show that the posterior then must also be a normal distribution? Then VI could even find the true parameters?)). 
  \item Potential scale reduction as a tool for diagnosing HMC convergence might not work very well for bayesian neural networks. (Potential scale reduction is never a sufficient criterion) It is just as sufficient for convergence as in other cases, but it is much less necessary. This is because there are so many weight space symmetries and it might not be necessary (or possible) to visit them all. Maybe a better metric would be to look at the predictive distributions' distance to each other since we only care about them.
  \item Should we include ``Single memeber better than ensemble'' experiments? Well we should mention somewhere that ensembles must be better than the average of the members in terms of log likelihood and RMSE. Then we can say: it is still possible that a single lucky member network is better than the ensemble. But we find this is mostly not the case. Can be an interesting sidenote, but only if it's not too much work. And it should probably be repeated on at least ~3 datasets. So probably it is too much work.
  \item mode changes under re parameterization argument against map networks. Bishop mentions this explicitly on page 18.
  \item LLB network/ensemble concern that it becomes overconfident when too much data is available. Toy Experiment apparently shows that this is not the case.
  \item I should mention that I put a lot of effort in using tensorflow probability's multi chain framework for the BNN with HMC so that multiple chains are really quick to compute. And for small networks indeed running e.g. 128 chains in parallel is only a few times slower then running only one chain.  For large networks however not so much is gained. \cite{lao2020tfp}
  \item Moment matching: Make a small note (possibly in the discussion) that some papers have found improvements by using a moment matched Gaussian instead of the mixture as a predictive distribution. Indeed, we also saw slight improvements most of the time by using a moment matched predictive distribution, but not all of the time. And the improvements were usually relatively small. Since moment matching is a bit of a hack we decided to report the results of the actual mixture distribution in this work.
  % \item Make a small note for UCI experiments: VI initially seemed to perform quite bad. So we did a little bit of hyper parameter tuning. in particular initialization of the mean of the variational posterior and initialization of the likelihoods noise scale. VI seemed to be especially sensitive to these parameters in the one hidden layer setting. We did not investigate this thoroughly and in the end we chose parameters that seemed to be best overall. Therefore VI can likely benefit from dataset specific hyper parameter tuning. No such tuning was performed for MAP and LLB networks and ensembles. We simply went with the hyper parameters that seemed promising from our toy experiments.
  \item My one hidden layer standard VI is better than Miguel's Probabilistic Backpropagation paper benchmark VI on every single dataset sometimes by a lot. And they used Bayesian optimization to find good hyper parameters (but only trained for 40 epochs). So at least VI's bad performance is not due to me having it implemented badly. !! doublecheck again if this is true if you want to include it in the thesis !!
  \item We use standard VI. If we used instead the re-parameterization trick or flipout, VI performance would likely be even better.
  \item Maybe list somewhere all hyper parameters that can be tuned for MAP and LLB networks and ensembles and VI.
\end{itemize}

\printbibliography


\begin{appendices}
\subfile{chapters/07_appendices}
\end{appendices}


\end{document}
