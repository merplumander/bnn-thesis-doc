\documentclass[../thesis.tex]{subfiles}

\begin{document}




% (predictive distribution convergence?)
% (not only avergae log likelihood but also worst case or best case log likelihood or also looking at quantiles 0-25\%, 25-75\% and 75+)
% How different are the members predictive distributions to eacht other?\cite{fort2019deep}

% Given the supposed great complexity of Bayesian neural network posteriors (e.g. because of weight space symmetries), it seems surprising that five or ten ensemble members are enough. But the natural question is what do we mean by enough. Apparently the performance (in terms of log likelihood and mean squared error) does not improve with more members, but is that because we have approximated the true bayesian neural network posterior sufficiently well, or at least the true posterior predictive distribution? Have we even reached the performance of the true posterior predictive distribution (let alone its true form)? In the following chapter we will compare ensembles to bayesian neural networks approximated by MCMC sampling to understand these questions in more detail.

% Have we approximated the bayesian neural network's predictive distribution or performance yet? 
% What is the convergence behaviour of samples from the (HMC) posterior? Are MAP networks / ensembles in some sense better than samples from the posterior? Since they are the modes of the dist. they might represent it better.


\end{document}