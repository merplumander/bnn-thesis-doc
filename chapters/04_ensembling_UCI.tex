\documentclass[../thesis.tex]{subfiles}

\begin{document}

So far this thesis has introduced neural linear ensembles as an approximative method that builds on deep ensembles and trades a small amount of additional computation for potentially enhanced epistemic uncertainty estimation.
Naturally, the next question is whether this translates to improved empirical performance on real-world data. Although ensembling has a long history in machine learning, using an ensemble of probabilistic neural networks to improve uncertainty estimation in deep learning has only recently attracted considerable attention and so there are still important knowledge gaps to be filled even for MAP ensembles. We, therefore, consider the following research questions which are aimed at examining unknown aspects of the behavior of deep neural network ensembles generally as well as understanding which benefits (if any) NL ensembles offer in practice. 
\medskip

\noindent \textbf{Question 1}: How much performance gains over a single network can be realized by using an ensemble? We know that ensembles must have a better (or equal) performance than the average performance of their members\footnote{It is unclear though, whether the best-performing member is better than the ensemble. We do not consider this question in detail, but have some initial evidence showing that this is not consistently the case.}. Yet, adding a second network to a single one (and thus creating an ensemble) must not necessarily improve upon the first network in every case depending on the performance of the second, but on average a two-member ensemble should be better than a single network. Exactly how large the performance gain is, is an empirical question. 


\medskip

\noindent \textbf{Question 2}: When working with ensembles, a natural and practically important question is how many members to train. Given that many different functions can usually explain real-world data well and that neural networks with many parameters can represent quite a range of different functions, it seems likely that neural network posteriors have many functionally different modes. %On the other hand, in high dimensional spaces (like the parameter space of a neural network) it may be unlikely to get actual local optima which are not saddle points since local optima must be ``walled off'' in exponentially many directions.
While there seem to exist paths of low loss between these modes \parencite{draxler2018essentially} they still appear to be functionally different \parencite{fort2019deep}. Therefore, it seems like there is quite some room for additional ensemble members to possibly increase performance. Whether the additional networks consistently find functionally different modes is not clear though. In any case we expect diminishing returns at some point.
Initial evidence in the literature seems to suggest that 5 to 10 members are enough for good performance and further adding of members does not help much. \textcite{lakshminarayanan2017simple} introduced deep ensembles and note that when increasing the number of members ``accuracy and the quality of predictive uncertainty improve significantly''. But they only considered a maximum ensemble size of 15 for small experiments and report most results for 5 members. In the appendix of \textcite{ovadia2019can} ensembles of up to size 50 are compared on image classification tasks (CIFAR-10 and ImageNet) including also scenarios with dataset shift (Gaussian blur) and the authors conclude that ``increasing the number of models in the ensemble improves performance with diminishing returns beyond size 5''. 
It is however unclear, how the returns of using larger ensembles diminish precisely as a function of ensemble size, how consistent this functional relationship is across datasets and if there exists a point of convergence above which no additional gains can be realized by adding more members. 
\medskip

\noindent \textbf{Question 3}: The above questions concern ensembling in general. But how do NL ensembles perform in comparison to MAP ensembles? We have seen theoretical considerations about possible improvements of NL ensembles, but how does this translate to real-world datasets? 
\medskip

\noindent \textbf{Question 4}: Finally it would be instructive to compare the previously mentioned methods to standard BNN models to have an absolute point of comparison for the performance metrics and not just the relative gains among ensembles. For example, how are (MAP and NL) ensembles performing compared to VI BNNs?
\medskip

To measure the performance of the different BNN approximation methods, we use two metrics. One is the Root Mean Squared Error (RMSE), which measures the squared difference between the expected value of the predictive distribution and the actual data. RMSE is a measure of the accuracy of the mean prediction but neglects all uncertainty information. The other performance metric is the negative Log-Likelihood (LL) of the data under the predictive distribution of the model. This metric measures both the accuracy and the predictive uncertainty and similar to RMSE lower numeric values are better. A good prediction will be close to the actual test data point and have low uncertainty, while a bad prediction will be far off from the test data point and still have low predictive uncertainty. An intermediate value for the negative LL might, however, be caused by either an accurate prediction that is excessively uncertain or a somewhat less accurate prediction with reasonable uncertainty. %Discerning why one model scores better than another is therefore not always easy. 


\section{UCI Datasets}
To answer the research questions we benchmark the performance of MAP and NL networks and ensembles, and VI on nine standardly used, real-world regression datasets. Most of these datasets can be found on the UCI machine learning repository \parencite{dua2017uci} and they were used in several previous publications concerned with probabilistic neural networks \parencite{hernandez2015probabilistic, gal2016dropout, lakshminarayanan2017simple, ghosh2018structured, sun2019functional, ober2019benchmarking}. The datasets range from small to medium size of ${\sim}300$ data points for the ``yacht'' dataset and ${\sim}45000$ data points for ``protein''. They have an input dimensionality between 4 and 16. For each dataset we obtained the train-test splits used in \textcite{gal2016dropout} and \textcite{hernandez2015probabilistic}\footnote{Train-test splits available here: https://github.com/yaringal/DropoutUncertaintyExps}. There are 20 splits for all datasets except for ``protein'' for which there are only five splits.


\section{Model Architecture \& Baselines}
The architecture of all models consists of two hidden layers (one hidden layer results in the appendix) of size 50 with ReLU activations similar to previous papers\footnote{We also use 50 units in the hidden layers for ``protein'', unlike some previous papers that use 100 units for this dataset.}. We chose values for the other hyperparameters that seemed reasonable from prior knowledge or experiments on toy data.
%$10^{-6} \cdot \log(1 + e^{0.5x})$ and we initialize it to -1 which represents a standard deviation of about $0.47$. 
We use a factorized Gaussian with zero mean and unit variance as a prior for the weights and biases and an inverse gamma prior with $a=0.5$, $b=0.01$ for the noise variance. The latter distribution can easily be overcome by the data yet has 89\% of its mass below 1, which is desirable since we z-normalize the targets and so a variance above 1 seems unlikely\footnote{Another natural prior for the standard deviation is uniform between 0 and 1, but for this prior the Bayesian linear regression is not solvable in closed form. For consistency we therefore use the inverse gamma prior for all models.}.
In practice we square-root transform this distribution to be applicable to the standard deviation which makes a difference since we do a MAP estimate for this parameter and the location of the mode of a distribution is changing when using a nonlinear transformation like the square root \parencite[Chapter~1.2.1]{bishop2006pattern}. All models have an unconstrained variable defining the noise standard deviation through a softplus transformation to avoid negative values. We initialize the weights for all MAP networks with TensorFlow's implementation of Glorot uniform \parencite{glorot2010understanding} and VI's variational Gaussian posterior with means sampled from $\mathcal{N}(0, 0.1)$ and a small standard deviation as suggested by \textcite{hernandez2016black}. All models are trained with the Adam optimizer \parencite{kingma2014adam} until convergence defined by not improving their training objective for 20 epochs and use a learning rate of $0.01$ and a batch size of 100.

We take the predictive distribution of an ensemble to be the equally weighted mixture of the individual predictive distributions. Some previous papers have instead used a Gaussian with its moments---the mean and variance---matched to the mixture distribution and reported this to be an improvement \parencite{lakshminarayanan2017simple}. In line with this we also often saw slight improvements when using moment matching but not fully consistently. Since moment matching is a trick to simplify the predictive distribution and and performance enhancements were not constantly present, we decided to report the results of the mixture distribution instead.


\section{Results}
For each split of every dataset, we train VI once, 50 MAP networks, and 50 NL networks with the feature extractor weights taken from the corresponding MAP network. We then build ensembles of up to size 50 out of the single networks. Here the larger ensembles comprise all networks of the smaller ensembles which shows us the benefits that are important in practice, namely how the performance changes when enlarging the existing ensemble. In figure \ref{fig:uci-benchmark-nll} and figure \ref{fig:uci-benchmark-rmse}, we plot the negative average test Log-Likelihood (LL) and Root Mean Squared Error (RMSE) respectively for VI, and MAP and NL networks and ensembles with two hidden layers (one hidden layer results in appendix \ref{appendix:uci-results}). We show the performance of ensemble sizes 5, 20, and 50 here. 
 %Unlike previous papers, we report all performance metrics in normalized y-space. This has the advantage of making metrics like RMSE more interpretable without going into the details of each dataset and more comparable across datasets. To ensure comparability with previous work we report the results for unnormalized y-space in appendix \ref{appendix:uci}.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{uci_two-hidden-layers_NLLs} 
    % \setlength{\figwidth}{.2\textwidth}
    % \setlength{\figheight}{.22\textheight}
    % \input{figures/uci_two-hidden-layers_NLLs.tex}
    \caption{Benchmark of different BNN approximations on the nine UCI datasets. We compare the performance of VI (green squares), MAP networks (blue triangles), MAP ensembles of different sizes (purple circles), and NL networks (yellow triangles) and NL ensembles (orange circles) of different sizes in terms of the negative average test log-likelihood across the dataset splits. Shown are mean and standard error. The plot shows the performance of ensembles of sizes 5, 20, and 50 in increasing order.}
    \label{fig:uci-benchmark-nll}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{uci_two-hidden-layers_RMSEs} 
    \caption{Benchmark of different BNN approximations on the nine UCI datasets. We compare the performance of VI (green squares), MAP networks (blue triangles), MAP ensembles of different sizes (purple circles), and NL networks (yellow triangles) and NL ensembles (orange circles) of different sizes in terms of the average test root mean squared error (RMSE) across the dataset splits. Shown are mean and standard error. The plot shows the performance of ensembles of sizes 5, 20, and 50 in increasing order.}
    \label{fig:uci-benchmark-rmse}
\end{figure}

As expected single networks (both MAP and NL) perform worse in terms of both metrics compared to ensembles on all data sets. This does not only hold for the ensemble sizes shown but for all ensemble sizes and also for the one hidden layer benchmark. It does not hold on every single data split though (not shown). Even the small ensembles of size five tend to improve performance a lot above single networks and might therefore be an interesting option for practitioners.

Comparing the negative LL performance of MAP and NL ensembles we find that the latter do improve upon the former on some datasets sometimes even by a lot (for example on ``naval'' and ``yacht''). On ``boston'' and ``concrete'' they perform slightly worse though. Looking at RMSE performance, the differences are not as pronounced, and on ``boston'' and ``wine'' there is hardly a difference. On the other datasets, NL ensembles tend to have a slight advantage.

In line with previous work, the figures also show that larger ensembles perform better in terms of negative LL but that returns are indeed quickly diminishing above five members. Subsequently, we will analyze this behavior in more detail. Interestingly, the returns of using larger ensembles seem to follow a similar pattern whether we consider MAP or NL networks. Relatedly, the difference between the two ensemble types seems to be relatively stable across different ensemble sizes within each dataset. These findings seem qualitatively similar for RMSE performance.

Focusing on VI's negative LL performance, we find that it compares quite variably to the other methods. On ``wine'' it is on par with ensembles of size 50 while on several other datasets it is quite behind the other methods. For RMSE, VI's performance is worse with VI being the worst performing model on all but two datasets. One possible reason for VI's comparatively bad performance is its strong dependence on the initialization of the likelihood's noise scale and the variational posterior parameters which we did not tune. In toy experiments, we saw strongly differing performances depending on these hyperparameters. 
% We also considered our implementation to be suboptimal, but comparing the performance of our one-hidden-layer variational inference network to the performance in \textcite{hernandez2015probabilistic} we found that our implementation performed better for both negative LL and RMSE
%even though they performed a Bayesian optimization to find good hyperparameters (but on the other hand restricted the number of training epochs to 40)
% \footnote{For ``protein'' this is not a valid comparison since their hidden layer has 100 neurons while our hidden layer has 50 neurons.}. This at least suggests that VI's problems do not originate from details of our implementation.  Is it rude to mention this? All I want to do is convince the reader that my VI implementation is not flawed.. 





\subsection*{Returns of Using Larger Ensembles}
The previous section compared the performance of MAP and NL ensembles with VI, and single networks and found ensembling to be quite beneficial for both MAP and NL networks. We saw that returns are quickly diminishing, that 20 members are not that much better than 5, and that 50 members hardly improve upon 20. Previous research loosely noted that returns diminish above five members. It remains to be examined more quantitatively how the returns of larger ensembles unfold as a function of the ensemble size, how stable this pattern is across datasets, and whether there exists a point of convergence above which larger ensembles do not improve performance and which can therefore be regarded as representing the gains achievable by ensembling.

We aim to answer these questions with the following experiment. For each split of each of the UCI datasets, we benchmarked the performance of MAP and NL ensembles up to size 50 as above and compute average performances across the splits within a dataset. Next we scale these average performance curves such that single networks' performance lies at 0 and full 50-member ensembles' performance lies at 1 (using a min-max kind of scaling $y_{\text{scaled}} = (y - \text{min}) / (\text{max} - \text{min})$, where min is the performance of a single network and max is the performance of the full 50-member ensemble). Figure \ref{fig:uci-ensemble-sizes-ll} shows the scaled log-likelihood performance curves for each dataset for MAP and NL ensembles while figure \ref{fig:uci-ensemble-sizes-rmse} shows RMSE performances. The y-values represent the fraction of performance gains of 50-member ensembles over single networks achieved by the different ensemble sizes. 

\begin{figure}
    \centering
    \begin{subfigure}{0.49\textwidth}
        % \includegraphics[width=\linewidth]{uci_ensemble-size-convergence_two-hidden-layers_Ensemble_NLLs} 
        \setlength{\figwidth}{.99\textwidth}
        \setlength{\figheight}{.33\textheight}
        \input{figures/uci_ensemble-size-convergence_two-hidden-layers_Ensemble_NLLs.tex}
        % \caption{}
        \label{fig:uci-ensemble-sizes-ll-map}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        % \includegraphics[width=\linewidth]{uci_ensemble-size-convergence_two-hidden-layers_LLB-Ensemble_NLLs} 
        \setlength{\figwidth}{.99\textwidth}
        \setlength{\figheight}{.33\textheight}
        \input{figures/uci_ensemble-size-convergence_two-hidden-layers_LLB-Ensemble_NLLs.tex}
        % \caption{}
        \label{fig:uci-ensemble-sizes-ll-llb}
    \end{subfigure}
    \caption{Average LL performance curves for each dataset scaled such that single networks' performance lies at 0 and 50-member ensembles' performance lies at 1 (in light colors) for MAP ensembles (left) and NL ensembles (right). Average of the dataset curves in black. y-values represent fraction of performance gains of 50-member ensembles over single networks achieved by different ensemble sizes. Black dots mark ensemble sizes 2, 5, 10, 20, and 50 and dashed lines connect to y-axis.}
    \label{fig:uci-ensemble-sizes-ll}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{0.49\textwidth}
        % \includegraphics[width=\linewidth]{uci_ensemble-size-convergence_two-hidden-layers_Ensemble_RMSEs} 
        \setlength{\figwidth}{.99\textwidth}
        \setlength{\figheight}{.33\textheight}
        \input{figures/uci_ensemble-size-convergence_two-hidden-layers_Ensemble_RMSEs.tex}
        % \caption{}
        \label{fig:uci-ensemble-sizes-rmse-map}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        % \includegraphics[width=\linewidth]{uci_ensemble-size-convergence_two-hidden-layers_LLB-Ensemble_RMSEs} 
        \setlength{\figwidth}{.99\textwidth}
        \setlength{\figheight}{.33\textheight}
        \input{figures/uci_ensemble-size-convergence_two-hidden-layers_LLB-Ensemble_RMSEs.tex}
        % \caption{}
        \label{fig:uci-ensemble-sizes-rmse-llb}
    \end{subfigure}
    \caption{Average RMSE performance curves for each dataset scaled such that single networks' performance lies at 0 and 50-member ensembles' performance lies at 1 (in light colors) for MAP ensembles (left) and NL ensembles (right). Average of the dataset curves in black. y-values represent fraction of performance gains of 50-member ensembles over single networks achieved by different ensemble sizes. Black dots mark ensemble sizes 2, 5, 10, 20, and 50 and dashed lines connect to y-axis.}
    \label{fig:uci-ensemble-sizes-rmse}
\end{figure}

The figures show that returns of using larger ensembles follow the same pattern for MAP and NL ensembles. When comparing the black average curves in the same plot (not shown), the curves for MAP and NL are visually indistinguishable for both LL and RMSE. The return patterns are also remarkably similar across datasets with ``naval'' and ``yacht'' being a bit off at times but the others strongly agreeing. The ``yacht'' curve is above 1.0 for some ensemble sizes, most notably in figure \ref{fig:uci-ensemble-sizes-rmse} (right). This is simply due to medium-sized ensembles performing slightly better than 50-member ensembles in this case as can also be seen in figure \ref{fig:uci-benchmark-rmse}.

As expected we also see quickly diminishing returns in the number of members. There seems to be hardly any improvement in the region preceding 50 members and we will later see that going from 50 to \numprint{1000} members hardly changes the performance of an ensemble. Therefore we can regard the 50-member ensembles as approximately representing all gains we can achieve from ensembling neural networks in this way\footnote{Possibly we would see significant performance gains again when training much more members---one million for instance---but this seems hardly relevant for practice.}. In these experiments 2, 5, 10, and 20 networks reach on average 0.52, 0.82, 0.91, and 0.96 of the LL performance of the 50-member ensemble for MAP networks and 0.51, 0.81, 0.9, and 0.96 for NL networks (and 0.44, 0.79, 0.91, and 0.97 (MAP) and 0.45, 0.79, 0.91, and 0.97 (NL) for RMSE). We saw in the previous experiment that returns of ensembling over single networks seem large. And on average adding a second network gives about half the gains achievable from large ensembles not only for the uncertainty-aware metric LL but also for RMSE. Together, this makes a strong case for training at least a second network in practice to benefit from some of the possible performance gains.

Benchmarks on the UCI datasets measure the performance on i.i.d.\ test data. Another practically important setting is, however, to measure the performance on shifted test data since shifts in the distribution of the inputs $\vect{X}$ are hard to rule out in many applications. Recently, \textcite{foong2019between} introduced the ``UCI gap'' dataset splits. These are splits where the training data comprises the outer edges of the dataset along each input dimension respectively, while the test data lies in the middle. We repeated the experiments above with these data splits to measure how the models perform under distributional shift. The results can be found in appendix \ref{appendix:uci-gap}.


This chapter was concerned with questions of how ensembles of neural networks perform. We investigated whether adding a Bayesian linear regression to the last layer of all networks in an ensemble has a beneficial effect, how large the gains of ensembling are, and how they behave as a function of the number of members. The experiments indicate that NL ensembles generally behave quite similarly to MAP networks but can have a beneficial effect on performance. In line with theoretical results and previous research, we found that ensembles generally show strong improvements above single networks. The functional relationship of performance gains to the ensemble size shows quickly diminishing returns and indicates that on average even two-member ensembles show strong performance gains over single networks that represent approximately 50\% of all gains attainable by ensembling. The next chapter will dig deeper into why the behavior of ensembles is the way it is and how they compare to ``ground truth'' posteriors found by MCMC.


\end{document}
