\documentclass[../thesis.tex]{subfiles}

\begin{document}

\section{Existing BNN Approximations}

Since posterior distributions for Bayesian neural networks are generally intractable, we need to resort to approximative techniques. A large number of these methods have been developed in the last decades and so we will only review the most important ones here.

\subsection{Markov Chain Monte Carlo Sampling}
For many use cases having a large number of samples from the posterior distribution is just as good as knowing the distribution itself. With samples we can, for example, compute moments and quantiles of the distribution and, in a regression setting, we can use the samples for prediction. Markov Chain Monte Carlo (MCMC) methods are able to provide (dependent) samples from a posterior even if only the unnormalized posterior density $p(\vect{y}|\vect{\theta})p(\vect{\theta})$ is known. Luckily, this unnormalized density is usually readily available. It is simply the product of the likelihood of the parameter values $\vect{\theta}$ and their prior. So we need not consider the normalization constant $\int_{\vect{\theta}} p(\vect{y}|\vect{\theta})p(\vect{\theta})$ that is generally the intractable part of the posterior. To obtain samples these methods use a random walk in parameter space. From a given sample $\vect{\theta}_t$ a candidate $\vect{\theta}_{c}$ for the next sample is suggested from a proposal distribution, e.g.\ an isotropic normal distribution $\mathcal{N}(\vect{\theta}_t, \vect{I})$ centered at $\vect{\theta}_t$. This candidate is then accepted or rejected probabilistically based on the unnormalized density at both locations. If it is accepted, $\vect{\theta}_c$ becomes the next sample $\vect{\theta}_{t+1}$. Else the previous sample is used as a sample again $\vect{\theta}_{t+1} = \vect{\theta}_t$.
\subsubsection{Hamiltonian Monte Carlo}
In high dimensional distributions with possibly complicated shapes like neural network posteriors, a fixed proposal distribution often leads to inefficient sampling. Either the proposal distribution is too wide and most proposed samples are rejected since they fall into regions of low density or the proposal distribution is too narrow and the chain can only explore the distribution in tiny steps which necessitates sampling for longer to ensure that all parts of the distribution have been visited. In these cases, using Hamiltonian Monte Carlo (HMC) is sensible \parencite{duane1987hybrid, neal1995bayesian}. HMC replaces the fixed proposal distribution by gradient-based traversing of the distribution. It uses a momentum term and the gradient of the negative log posterior to take a number of so called leapfrog steps in weight space to arrive at a new proposed sample. This way, the proposal procedure adapts locally to the shape of the distribution and can generate higher acceptance rates while still exploring the distribution efficiently. With HMC we can approximate BNN posteriors closely, but this method is resource intensive even for small networks and datasets and becomes prohibitively expensive in larger settings.

\subsection{Variational Inference}
Variational Inference (VI) casts the task of approximating posterior distributions as an optimization problem. The procedure starts with a family of parametric, so-called variational distributions $q(\vect{\theta})$ (e.g.\ diagonal Gaussians) and searches for a set of parameters that minimizes some distance function to the true posterior $p(\vect{\theta}|\vect{D})$. The most common distance function used is the Kullback-Leibler (KL) divergence !! citation. This divergence is an asymmetric measure of the difference between two probability distributions $q$ and $p$, meaning that $\kl[q||p]\neq \kl[p||q]$, and does thus not qualify as a metric. $\kl[q||p]=0$ does, however, imply that the two distributions are identical. Two different approaches arise depending on which KL divergence is used. Variational Bayes \parencite{attias1999inferring}, the more standardly used method, works with $\kl[q||p]$ whereas expectation propagation \parencite{minka2001expectation} works with $\kl[p||q]$. A unifying framework arises when one considers $\alpha$-divergences between $p$ and $q$ of which both KL divergences are special cases \parencite{hernandez2016black}. In the following, we will give an overview of variational Bayes:
A natural question that arises is: How do we compute the KL divergence between the variational distribution $q$ and the true posterior $p(\vect{\theta} | \vect{y})$ that we do not know? We will see in the following that it is sufficient to know the unnormalized posterior $\tilde{p} = p(\vect{y} | \vect{\theta}) p(\vect{\theta})$.
\begin{align}
    \kl[q(\vect{\theta})||p(\vect{\theta} | \vect{y})] &= \int_{\vect{\theta}} q(\vect{\theta}) \log \frac{q(\vect{\theta})} {p(\vect{\theta} | \vect{y})} \\
    &= \int_{\vect{\theta}} q(\vect{\theta}) \log \frac{q(\vect{\theta})}  {\frac{p(\vect{y} | \vect{\theta}) p(\vect{\theta})} {p(\vect{y})}} \\
    &= \int_{\vect{\theta}} q(\vect{\theta}) \log \frac{q(\vect{\theta})}{p(\vect{y} | \vect{\theta}) p(\vect{\theta})}  +  \int_{\vect{\theta}} q(\vect{\theta}) p(\vect{y}) \\
    &= \int_{\vect{\theta}} q(\vect{\theta}) \log \frac{q(\vect{\theta})}{p(\vect{y} | \vect{\theta}) p(\vect{\theta})}  +   p(\vect{y}) \\
\iff p(\vect{y}) &= \kl[q(\vect{\theta})||p(\vect{\theta} | \vect{y})]  -  \int_{\vect{\theta}} q(\vect{\theta}) \log \frac{q(\vect{\theta})}{p(\vect{y} | \vect{\theta}) p(\vect{\theta})}
\end{align}
!! explain some steps? !!
!! define the ELBO in the equation above with curly braces? !!
Since the evidence $p(\vect{y})$ is a constant w.r.t.\ the parameters $\vect{\theta}$ we can minimize the KL divergence between the variational distribution and the true posterior $\kl[q(\vect{\theta})||p(\vect{\theta} | \vect{y})]$ by maximizing the \textit{evidence lower bound} (sometimes called the variational lower bound) $-\int_{\vect{\theta}} q(\vect{\theta}) \log \frac{q(\vect{\theta})}{p(\vect{y} | \vect{\theta}) p(\vect{\theta})}$ that only depends on the unnormalized posterior $\tilde{p}$. Also since the KL divergence cannot be negative, the evidence lower bound indeed bounds $p(\vect{y})$ from below.
!! Explain further how the ELBO is split into a likelihood and a prior term and how to approximate those !!

- Laplace
- (Maybe Dropout)

\subsection{Maximum a Posteriori Approximation}
A standard approximation that is used in machine learning is to consider only point estimates instead of distributions for the parameters of a model. This is a strongly simplifying move, but it allows much more efficient calculation of a result. A candidate for a good point estimate is the mode of the posterior and is known as the Maximum a Posteriori (MAP) estimate.
\begin{align}
    \vect{\theta}_{\mathrm{MAP}} &= \argmax_{\vect{\theta}} p(\vect{\theta} | \vect{y}) \\
    &= \argmax_{\vect{\theta}} p(\vect{y} | \vect{\theta}) p(\vect{\theta}) \\
\end{align}
Luckily, the normalization factor $p(\vect{y})$ is constant w.r.t.\ $\vect{\theta}$, does therefore not change the location of the maximum, and we can focus instead on the unnormalized posterior $p(\vect{y} | \vect{\theta}) p(\vect{\theta})$. If the prior is disregarded as well, the focus will lie solely on the likelihood and and the resulting parameter value $\argmax_{\vect{\theta}} p(\vect{y} | \vect{\theta})$ is known as the Maximum Likelihood (ML) estimate. In nonlinear models like neural networks, it is usually still not straightforward to find either of the estimates. Instead, an iterative procedure like gradient ascent is usually employed. But we still need to find the gradients of the unnormalized posterior w.r.t.\ the parameters $\vect{\theta}$. We can use the fact that applying the logarithm to a function does not change the location of the maximum, since the logarithm is a monotonically increasing function. Additionally, the data is often assumed to be independent and identically distributedI (i.i.d.), and therefore the likelihood is a product over individual data terms. 
\begin{align}
    \argmax_{\vect{\theta}} p(\vect{y} | \vect{\theta}) p(\vect{\theta}) &= \argmax_{\vect{\theta}} \log \left( p(\vect{y} | \vect{\theta}) p(\vect{\theta}) \right) \\
    &= \argmax_{\vect{\theta}} \log \left( \prod_{i=1}^N p(y_i | \vect{\theta}) p(\vect{\theta}) \right) \\
    &= \argmax_{\vect{\theta}} \sum_{i=1}^N \log p(y_i | \vect{\theta})  +  \log p(\vect{\theta})
\end{align}
Now finding the gradients of course depends on the assumptions about the data generating likelihood and the prior, but automatic differentiation, built into most modern machine learning packages, can usually take care of this. Iterative procedures like gradient ascent are usually not guaranteed to find the global maximum of a function, but instead return a local maximum. Nevertheless, these procedures lie at the heart of deep learning and its successes in the past decade. From a Bayesian perspective using point estimates for the parameters is equivalent to approximating the posterior distribution by a Dirac delta distribution. 



- MAP Ensembles -> State-of-the-art
    -... Ensembles have had successes in representing epistemic uncertainty and are state of the art currently. They manage to represent predictive uncertainty within the i.i.d. bounds of the dataset but also under distributional shift.
    -One generally useful fact is that an ensembles prediction on a single point must have a lower squared error than the average squared error of the individual members. (Jensen's Inequality). And lower Log Likelihood.
    What about looking at several data points at once? Ensemble must still be better than average.
    Related question: Is there a single lucky network that is better than the ensemble? 
    We have some data here. Should we include it? If yes, Where? (I don't think so)
- LLB / Neural Linear -> Simplicity, low computational effort

\end{document}