\documentclass[../thesis.tex]{subfiles}

\begin{document}


The field of deep learning has seen impressive progress in the last decade. From computer vision \parencite{krizhevsky2012imagenet} and natural language processing \parencite{brown2020language} over reinforcement learning \parencite{mnih2015human} to computational biology \parencite{senior2020improved}, neural networks show unrivaled performance in many machine learning application areas. But while the predictions of deep nets are mostly accurate on average, assessing the model's confidence in its prediction for individual data points is hard \parencite{guo2017calibration}. As a result, deep learning is usually applied in areas where making mistakes is comparatively cheap. As machines automate more and more high-stakes decision processes this issue becomes more problematic though. For autonomous driving for instance it is vitally important that the machine knows how certain it is of its prediction. In such a safety-critical application, erring on the safe side is prudent when predictions like ``no pedestrian is crossing the street'' cannot be made with high confidence.

One key reason for the lack of calibrated predictive uncertainties in deep learning is the neglect of epistemic uncertainty---the uncertainty about the values of the parameters and the corresponding functions implicitly induced by them \parencite{wilson2020case}. Neural networks are commonly trained by searching for the point estimate of the parameter values that minimizes a loss function. Other parameter values whose induced functions explain the data similarly well are not taken into account. 

Applying probability theory to inferring parameter values from data instead leads to a posterior distribution that expresses the uncertainty about the parameter values and represents how well each set of parameter values is in line with prior knowledge and the observed data. This approach is often called Bayesian \parencite[p.~xxvii]{murphy2012machine} and is markedly different from finding best fitting point estimates. In the Bayesian setting, the posterior predictive distribution arises by averaging the predictive distributions of all possible parameter values weighted by their posterior probability. It is precisely this incorporation of epistemic uncertainty into the prediction made by the model, that carries the prospect of resolving the problem of overconfident predictions in neural networks.

Applying Bayesian inference to neural networks promises useful predictive uncertainty and even enhanced predictive accuracy \parencite{wilson2020case}. However, Bayesian neural networks suffer from some problems in practice. Neural networks are often thought of as black-box function approximators and formulating prior knowledge we might have about the distribution of functions in terms of a prior for the parameters is hence a difficult problem. Additionally, computing the posterior distribution is intractable for neural networks and we have to resort to approximative techniques. Early approaches to approximating neural network posteriors came long before the ``deep learning revolution’’ \parencite{mackay1992practical, neal1995bayesian} but the field has since seen renewed interest focusing especially on variational approaches \parencite{graves2011practical, hernandez2015probabilistic, blundell2015weight}. 

Recently, the idea of ensembling deep neural networks has gained popularity \parencite{lakshminarayanan2017simple} and has been given a Bayesian interpretation \parencite{wilson2020case}. The key idea is simple yet effective. Deep neural networks trained from different initializations tend to find functionally different but high-performing solutions and can therefore represent some epistemic uncertainty in function space. Empirically deep ensembles have been found to provide state-of-the-art predictive uncertainties also in the practically important setting of dataset shift \parencite{ovadia2019can}. Nevertheless, there are still many open questions about deep ensembles. It is, for example, unclear how the gains of ensembling unfold as a function of the ensemble size and how stable this relationship is across datasets and architectures, how ensemble predictions relate to the ground truth Bayesian posterior predictive, and whether there are straightforward ways of enhancing standard deep ensembles which have not been explored yet.

This thesis aims at refining our understanding of ensembles of deep neural networks. We will first rehearse some necessary background in chapter 2. Chapter 3 will introduce a new approach to approximate Bayesian neural networks with ensembles that combines the idea of ensembling with another recently popular method of replacing the last layer with a Bayesian linear regression that allows to capture more epistemic uncertainty for these weights. In chapter 4 the thesis will examine how these ``Last Layer Bayesian ensembles'' perform on real-world data, especially compared to standard ensembles and how the gains of ensembling neural networks unfold as a function of the ensemble size. Subsequently, we will examine how the predictions made by neural network ensembles compare to the ``ground truth'' Markov chain Monte Carlo posterior predictive distribution in chapter 5. 


\end{document}