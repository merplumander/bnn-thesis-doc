\documentclass[../thesis.tex]{subfiles}

\begin{document}

Two viewpoint metrics experiments.
+ Test LL vs. Log Posterior plot

It might be instructive to look at the convergence behaviour of posterior samples and map ensembles (when using more samples and members) in weight space, but it is not straightforward to do this comparison. We might use Wasserstein distance, but approximating Wasserstein distance with samples requires a ridiculously large amount of samples in this high dimensional space. But for neural networks we actually don't care so much about the weight space anyway.

Checking for convergence of Markov chain Monte Carlo chains for bayesian neural networks should focus on the space of predictive distributions and not the weight space. (The weight space of a network of [1, 100, 1] units has 100! symmetries. Which can never be explored in a reasonable amount of time. So we expect to show standard potential scale reduction to diagnose non-converged chains even when the predictive distribtuion of the chains have already converged (and that's all we care about)) ->  potential scale reduction on the means and variance of the posterior predictive distribution on test points. And predictive distribution Wasserstein distance.
Find out whether this has been used before as a convergence criterion for MCMC bayesian neural networks. If not this both are a contribution!

% In our experiments we realized that single samples from the HMC posterior usually perform better than map networks. Has not really turned out to be true.

soap bubble argument: If we had the actual posterior we would make predictions by drawing samples from it and averaging their predictions (like we do with the HMC samples). Due to the soap bubble argument, MAP networks are not at all typical samples from the posterior in weight space ( and therefore probably also in function space). Should we expect map networks to perform better or worse than random samples from the posterior? Map networks are located at a peak of the posterior but what is that worth? We care about probability mass not probability density and this local spike might be very small in terms of volume. Second, when reparameterizing the model the modes generally change location.

% (Possibly, few hmc samples are enough to improve upon map network)
% If so, HMC cannot only provide the gold standard in terms of full posterior estimation, but it might also be seen as a tool (on top of map training) that can give us well performing ensembles at a low cost.

UCI Gap?

\end{document}