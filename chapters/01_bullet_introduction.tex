\documentclass[../thesis.tex]{subfiles}

\begin{document}

Probability theory provides mathematical framework for dealing with uncertainties. Cox, Jaynes $\rightarrow$ rational observer updates beliefs using probability theory.
\bigskip

Derive Bayes theorem generally. $\rightarrow$ Allows unobserved variables (like parameters of a model) to be inferred by data.
\bigskip

Conditional modelling (regression, classification) focusing on the distribution of $y|X$. Functions with global / local properties $\rightarrow$ what can we learn about the function from empirical data? $\rightarrow$ depending on our assumptions we can learn something about the function from data globally / locally (important for IID vs OOD)
\bigskip

Perhaps first introduce Bayesian linear regression. Can also be done classically with point estimates that can also be derived from error minimization. Through the bayesian lens a point estimate for the parameters can be viewed as using a Dirac delta distribution to approximate the posterior. This approximation can be good when the posterior is strongly peaked around its mode, which usually happens when we have much more data than parameters, or it can be bad e.g.\ when the posterior is multimodal, as can happen in non-linear models with many parameters (like neural networks) . Both Bayesian and classic linear regression can be solved in closed form. Point estimates plagued by overfitting and overconfident predictions.
\bigskip

Neural networks as flexible function approximators. Basis functions can adapt to data and act as a feature extractor. Empirically more useful than hand-engineered features. Last layer roughly acts as linear regression. MAP estimate can no longer be derived in closed form $\rightarrow$ A need to use iterative procedures like gradient ascent. Bayesian NNs hard to approximate. $\rightarrow$ classically use either MCMC or VI. In this work we take a closer look at ensembling methods through the Bayesian lens. !! cite: Lakshminarayanan, Ovadia, Wilson !!
\bigskip

Bayesian deep learning promises great predictions (no overfitting) and useful uncertainties (model knows when it does not know). Especially important when there is a shift in the dataset (distribution of $X$) at test time. $\rightarrow$ OOD Data (discuss difference between shift in distribution of $X$ vs.\ $y$ since we are modelling $p(y|X)$)

\end{document}